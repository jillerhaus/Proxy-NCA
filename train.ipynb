{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import imp\n",
    "import dataset\n",
    "import utils\n",
    "import proxynca\n",
    "import net\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from utils import JSONEncoder, json_dumps\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Args Class\n",
    "\n",
    "Class containing the parameters for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args():\n",
    "    dataset = 'food'\n",
    "    config = 'config1.json'\n",
    "    sz_embedding = 64 #size of the embedding that is appendet to inceptionv2\n",
    "    sz_batch = 32 #number of samples per batch\n",
    "    nb_epochs = 10\n",
    "    gpu_id = 0\n",
    "    nb_workers = 4\n",
    "    with_nmi = True  #turn calculations for nmi on or off turn off for sop\n",
    "    scaling_x = 3.0 #scaling factor for the normalized embeddings\n",
    "    scaling_p = 3.0 #scaling factor for the normalized proxies\n",
    "    lr_proxynca = 1.0 #learning rate for proxynca\n",
    "    log_filename = (f'''{dataset}-{dt.now().strftime(\"%Y%m%d-%H%M%S\")}''')\n",
    "    results_filename = f'{dataset}-results.csv'\n",
    "    torch_version = str(torch.__version__)\n",
    "    edition = 0\n",
    "    seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(args = args):\n",
    "    seed = args.seed\n",
    "    if seed != -1:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    else:\n",
    "        print('not seeded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(args.gpu_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_config(args = args):\n",
    "    config = utils.load_config(args.config)\n",
    "    config['criterion']['args']['scaling_x'] = args.scaling_x\n",
    "    config['criterion']['args']['scaling_p'] = args.scaling_p\n",
    "    config['opt']['args']['proxynca']['lr'] = args.lr_proxynca\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_tr(config = setup_config(), args = args):\n",
    "    dl_tr = torch.utils.data.DataLoader(\n",
    "        dataset.load(name = args.dataset,\n",
    "                     root = config['dataset'][args.dataset]['root'],\n",
    "                     classes = config['dataset'][args.dataset]['classes']['train'],\n",
    "                     transform = dataset.utils.make_transform(**config['transform_parameters'])\n",
    "                    ),\n",
    "        batch_size = args.sz_batch,\n",
    "        shuffle = True,\n",
    "        num_workers = args.nb_workers,\n",
    "        drop_last = True,\n",
    "        pin_memory = True\n",
    "    )\n",
    "    return dl_tr\n",
    "\n",
    "def load_ev(config = setup_config(), args = args):\n",
    "    dl_ev = torch.utils.data.DataLoader(\n",
    "        dataset.load(\n",
    "            name = args.dataset,\n",
    "            root = config['dataset'][args.dataset]['root'],\n",
    "            classes = config['dataset'][args.dataset]['classes']['eval'],\n",
    "            transform = dataset.utils.make_transform(\n",
    "                **config['transform_parameters'],\n",
    "                is_train = False)\n",
    "        ),\n",
    "        batch_size = args.sz_batch,\n",
    "        shuffle = False,\n",
    "        num_workers = args.nb_workers,\n",
    "        pin_memory = True\n",
    "    )\n",
    "    return dl_ev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(args = args):\n",
    "    model = net.bn_inception(pretrained = True)\n",
    "    net.embed(model, sz_embedding = args.sz_embedding)\n",
    "    model = model.cuda()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion\n",
    "\n",
    "This function initializes the training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_criterion(config = setup_config(), args = args, dl_tr = load_tr()):\n",
    "    criterion = proxynca.ProxyNCA(\n",
    "        nb_classes = dl_tr.dataset.nb_classes(),\n",
    "        sz_embedding = args.sz_embedding,\n",
    "        **config['criterion']['args']).cuda()\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_opt(config = setup_config(), model = setup_model(), criterion = setup_criterion()):\n",
    "    opt = config['opt']['type'](\n",
    "        [\n",
    "            { # inception parameters, excluding embedding layer\n",
    "                **{'params': list(\n",
    "                    set(\n",
    "                        model.parameters()\n",
    "                    ).difference(\n",
    "                        set(model.embedding_layer.parameters())\n",
    "                    )\n",
    "                )},\n",
    "                **config['opt']['args']['backbone']\n",
    "            },\n",
    "            { # embedding parameters\n",
    "                **{'params': model.embedding_layer.parameters()},\n",
    "                **config['opt']['args']['embedding']\n",
    "            },\n",
    "            { # proxy nca parameters\n",
    "                **{'params': criterion.parameters()},\n",
    "                **config['opt']['args']['proxynca']\n",
    "            }\n",
    "        ],\n",
    "        **config['opt']['args']['base']\n",
    "    )\n",
    "    return opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_scheduler(config = setup_config(), opt = setup_opt()):\n",
    "    scheduler = config['lr_scheduler']['type'](\n",
    "        opt, **config['lr_scheduler']['args'])\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(args = args):\n",
    "    imp.reload(logging)\n",
    "    logging.basicConfig(\n",
    "        format = \"%(asctime)s %(message)s\",\n",
    "        level = logging.INFO,\n",
    "        handlers = [\n",
    "            logging.FileHandler(\"{0}/{1}.log\".format('log', args.log_filename)),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logging.info(\"Training parameters: {}\".format(vars(args)))\n",
    "    logging.info(\"Training for {} epochs\".format(args.nb_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(args = args):\n",
    "    #set up new parameters\n",
    "    seed_everything(args)\n",
    "    config = setup_config(args)\n",
    "    dl_tr = load_tr(config, args)\n",
    "    dl_ev = load_ev(config, args)\n",
    "    model = setup_model(args = args)\n",
    "    criterion = setup_criterion(config = config, args = args, dl_tr = load_tr())\n",
    "    opt = setup_opt(config = config, model = model, criterion = criterion)\n",
    "    scheduler = setup_scheduler(config = config, opt = opt)\n",
    "    setup_logging(args = args)\n",
    "    \n",
    "    if args.with_nmi == True:\n",
    "        df = pd.DataFrame(columns = ['epoch', 'r@1', 'r@2', 'r@4', 'r@8', 'NMI'])\n",
    "    else:\n",
    "        df = pd.DataFrame(columns = ['epoch', 'r@1', 'r@2', 'r@4','r@8'])\n",
    "    \n",
    "    losses = []\n",
    "    t1 = time.time()\n",
    "    logging.info(\"**Evaluating initial model.**\")\n",
    "    with torch.no_grad():\n",
    "        utils.evaluate(model, dl_ev, with_nmi = args.with_nmi)\n",
    "\n",
    "    for e in range(0, args.nb_epochs):\n",
    "        if e!=0:\n",
    "            scheduler.step()\n",
    "        time_per_epoch_1 = time.time()\n",
    "        losses_per_epoch = []\n",
    "        for x,y, _ in dl_tr:\n",
    "            opt.zero_grad()\n",
    "            m = model(x.cuda())\n",
    "            loss = criterion(m, y.cuda())\n",
    "            loss.backward()\n",
    "\n",
    "#             torch.nn.utils.clip_grad_value_(model.parameters(), 10)\n",
    "            \n",
    "            losses_per_epoch.append(loss.data.cpu().numpy())\n",
    "            opt.step()\n",
    "\n",
    "        time_per_epoch_2 = time.time()\n",
    "        losses.append(np.mean(losses_per_epoch[-20:]))\n",
    "        logging.info(\n",
    "            \"Epoch: {}, loss: {:.3f}, time (seconds): {:.2f}.\".format(\n",
    "                e,\n",
    "                losses[-1],\n",
    "                time_per_epoch_2 - time_per_epoch_1))\n",
    "        with torch.no_grad():\n",
    "            logging.info(\"**Evaluating.**\")\n",
    "            recall = utils.evaluate(model, dl_ev, with_nmi = args.with_nmi) #variable name not accurate\n",
    "            # append results of current epoch to df\n",
    "            if args.with_nmi == True:\n",
    "                lst = recall[0].copy()\n",
    "                lst.append(recall[1])\n",
    "                lst.insert(0,e)\n",
    "                df_epoch = pd.DataFrame([lst], columns = ['epoch', 'r@1', 'r@2', 'r@4','r@8', 'NMI'])\n",
    "            else:\n",
    "                lst = recall.copy()\n",
    "                lst.insert(0,e)\n",
    "                df_epoch = pd.DataFrame([lst], columns = ['epoch', 'r@1', 'r@2', 'r@4', 'r@8'])\n",
    "            df = pd.concat([df,df_epoch])\n",
    "            model.losses = losses\n",
    "            model.current_epoch = e\n",
    "\n",
    "    t2 = time.time()\n",
    "    logging.info(\"Total training time (minutes): {:.2f}.\".format((t2 - t1) / 60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped version:0. It was already done.\n",
      "skipped version:1. It was already done.\n",
      "53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-20 21:41:47,320 Training parameters: {'__module__': '__main__', 'dataset': 'food', 'config': 'config1.json', 'sz_embedding': 64, 'sz_batch': 32, 'nb_epochs': 10, 'gpu_id': 0, 'nb_workers': 4, 'with_nmi': True, 'scaling_x': 1.0, 'scaling_p': 8.0, 'lr_proxynca': 1, 'log_filename': 'food-20200920-214144', 'results_filename': 'food-results.csv', 'torch_version': '1.1.0', 'edition': 2, 'seed': 0, '__dict__': <attribute '__dict__' of 'args' objects>, '__weakref__': <attribute '__weakref__' of 'args' objects>, '__doc__': None, 'sz_embeddings': 64}\n",
      "2020-09-20 21:41:47,323 Training for 10 epochs\n",
      "2020-09-20 21:41:47,324 **Evaluating initial model.**\n",
      "2020-09-20 21:42:28,009 NMI: 25.916\n",
      "2020-09-20 21:42:40,019 R@1 : 32.493\n",
      "2020-09-20 21:42:40,276 R@2 : 41.514\n",
      "2020-09-20 21:42:40,533 R@4 : 51.994\n",
      "2020-09-20 21:42:40,781 R@8 : 62.736\n",
      "2020-09-20 21:43:31,837 Epoch: 0, loss: 2.729, time (seconds): 51.06.\n",
      "2020-09-20 21:43:31,837 **Evaluating.**\n",
      "2020-09-20 21:44:11,373 NMI: 27.167\n",
      "2020-09-20 21:44:22,631 R@1 : 31.746\n",
      "2020-09-20 21:44:22,912 R@2 : 42.148\n",
      "2020-09-20 21:44:23,187 R@4 : 53.350\n",
      "2020-09-20 21:44:23,464 R@8 : 64.370\n",
      "2020-09-20 21:45:14,689 Epoch: 1, loss: 2.368, time (seconds): 51.22.\n",
      "2020-09-20 21:45:14,690 **Evaluating.**\n",
      "2020-09-20 21:45:54,309 NMI: 28.867\n",
      "2020-09-20 21:46:05,781 R@1 : 35.292\n",
      "2020-09-20 21:46:06,031 R@2 : 45.398\n",
      "2020-09-20 21:46:06,282 R@4 : 56.348\n",
      "2020-09-20 21:46:06,534 R@8 : 67.150\n",
      "2020-09-20 21:46:58,254 Epoch: 2, loss: 2.161, time (seconds): 51.72.\n",
      "2020-09-20 21:46:58,255 **Evaluating.**\n",
      "2020-09-20 21:47:37,871 NMI: 30.296\n",
      "2020-09-20 21:47:49,358 R@1 : 36.621\n",
      "2020-09-20 21:47:49,607 R@2 : 47.258\n",
      "2020-09-20 21:47:49,860 R@4 : 58.269\n",
      "2020-09-20 21:47:50,114 R@8 : 68.506\n",
      "2020-09-20 21:48:40,684 Epoch: 3, loss: 2.201, time (seconds): 50.57.\n",
      "2020-09-20 21:48:40,685 **Evaluating.**\n",
      "2020-09-20 21:49:19,465 NMI: 29.216\n",
      "2020-09-20 21:49:31,021 R@1 : 37.117\n",
      "2020-09-20 21:49:31,270 R@2 : 47.771\n",
      "2020-09-20 21:49:31,517 R@4 : 58.504\n",
      "2020-09-20 21:49:31,767 R@8 : 68.541\n",
      "2020-09-20 21:50:22,686 Epoch: 4, loss: 1.939, time (seconds): 50.92.\n",
      "2020-09-20 21:50:22,688 **Evaluating.**\n",
      "2020-09-20 21:51:01,730 NMI: 29.714\n",
      "2020-09-20 21:51:13,459 R@1 : 36.699\n",
      "2020-09-20 21:51:13,709 R@2 : 46.745\n",
      "2020-09-20 21:51:13,952 R@4 : 57.930\n",
      "2020-09-20 21:51:14,202 R@8 : 68.298\n",
      "2020-09-20 21:52:05,815 Epoch: 5, loss: 1.949, time (seconds): 51.61.\n",
      "2020-09-20 21:52:05,816 **Evaluating.**\n",
      "2020-09-20 21:52:44,354 NMI: 29.544\n",
      "2020-09-20 21:52:55,458 R@1 : 37.308\n",
      "2020-09-20 21:52:55,702 R@2 : 47.389\n",
      "2020-09-20 21:52:55,946 R@4 : 58.069\n",
      "2020-09-20 21:52:56,189 R@8 : 68.437\n",
      "2020-09-20 21:53:48,496 Epoch: 6, loss: 1.751, time (seconds): 52.31.\n",
      "2020-09-20 21:53:48,497 **Evaluating.**\n",
      "2020-09-20 21:54:28,335 NMI: 30.130\n",
      "2020-09-20 21:54:40,079 R@1 : 37.360\n",
      "2020-09-20 21:54:40,356 R@2 : 47.910\n",
      "2020-09-20 21:54:40,606 R@4 : 59.034\n",
      "2020-09-20 21:54:40,854 R@8 : 69.297\n",
      "2020-09-20 21:55:33,731 Epoch: 7, loss: 1.701, time (seconds): 52.87.\n",
      "2020-09-20 21:55:33,732 **Evaluating.**\n",
      "2020-09-20 21:56:10,979 NMI: 30.770\n",
      "2020-09-20 21:56:21,854 R@1 : 37.742\n",
      "2020-09-20 21:56:22,094 R@2 : 48.092\n",
      "2020-09-20 21:56:22,342 R@4 : 58.869\n",
      "2020-09-20 21:56:22,589 R@8 : 69.679\n",
      "2020-09-20 21:57:15,763 Epoch: 8, loss: 1.627, time (seconds): 53.17.\n",
      "2020-09-20 21:57:15,764 **Evaluating.**\n",
      "2020-09-20 21:57:52,503 NMI: 30.279\n",
      "2020-09-20 21:58:03,969 R@1 : 37.403\n",
      "2020-09-20 21:58:04,216 R@2 : 48.006\n",
      "2020-09-20 21:58:04,458 R@4 : 58.121\n",
      "2020-09-20 21:58:04,699 R@8 : 68.628\n",
      "2020-09-20 21:58:56,741 Epoch: 9, loss: 1.665, time (seconds): 52.04.\n",
      "2020-09-20 21:58:56,741 **Evaluating.**\n",
      "2020-09-20 21:59:33,579 NMI: 30.243\n",
      "2020-09-20 21:59:44,516 R@1 : 37.629\n",
      "2020-09-20 21:59:44,763 R@2 : 47.736\n",
      "2020-09-20 21:59:45,007 R@4 : 58.981\n",
      "2020-09-20 21:59:45,255 R@8 : 69.549\n",
      "2020-09-20 21:59:45,257 Total training time (minutes): 17.97.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-20 21:59:46,029 Training parameters: {'__module__': '__main__', 'dataset': 'food', 'config': 'config1.json', 'sz_embedding': 64, 'sz_batch': 128, 'nb_epochs': 10, 'gpu_id': 0, 'nb_workers': 4, 'with_nmi': True, 'scaling_x': 1.0, 'scaling_p': 8.0, 'lr_proxynca': 1, 'log_filename': 'food-20200920-214144', 'results_filename': 'food-results.csv', 'torch_version': '1.1.0', 'edition': 2, 'seed': 0, '__dict__': <attribute '__dict__' of 'args' objects>, '__weakref__': <attribute '__weakref__' of 'args' objects>, '__doc__': None, 'sz_embeddings': 64}\n",
      "2020-09-20 21:59:46,029 Training for 10 epochs\n",
      "2020-09-20 21:59:46,031 **Evaluating initial model.**\n",
      "2020-09-20 22:00:20,780 NMI: 25.916\n",
      "2020-09-20 22:00:32,409 R@1 : 32.493\n",
      "2020-09-20 22:00:32,662 R@2 : 41.514\n",
      "2020-09-20 22:00:32,917 R@4 : 51.994\n",
      "2020-09-20 22:00:33,174 R@8 : 62.736\n",
      "2020-09-20 22:01:07,768 Epoch: 0, loss: 3.464, time (seconds): 34.59.\n",
      "2020-09-20 22:01:07,769 **Evaluating.**\n",
      "2020-09-20 22:01:42,199 NMI: 23.528\n",
      "2020-09-20 22:01:52,958 R@1 : 30.277\n",
      "2020-09-20 22:01:53,214 R@2 : 40.280\n",
      "2020-09-20 22:01:53,471 R@4 : 51.195\n",
      "2020-09-20 22:01:53,728 R@8 : 63.170\n",
      "2020-09-20 22:02:27,801 Epoch: 1, loss: 2.689, time (seconds): 34.07.\n",
      "2020-09-20 22:02:27,801 **Evaluating.**\n",
      "2020-09-20 22:03:01,762 NMI: 28.252\n",
      "2020-09-20 22:03:12,519 R@1 : 34.761\n",
      "2020-09-20 22:03:12,761 R@2 : 45.346\n",
      "2020-09-20 22:03:13,020 R@4 : 56.192\n",
      "2020-09-20 22:03:13,265 R@8 : 67.107\n",
      "2020-09-20 22:03:47,293 Epoch: 2, loss: 2.326, time (seconds): 34.03.\n",
      "2020-09-20 22:03:47,294 **Evaluating.**\n",
      "2020-09-20 22:04:21,361 NMI: 30.807\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-cc32b9077e92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m                                     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                                 \u001b[0mres_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m                                 \u001b[0mres_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                                 \u001b[0mres_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'scl_x'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscl_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-c4490a337177>\u001b[0m in \u001b[0;36mtrain_and_test\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"**Evaluating.**\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_ev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_nmi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_nmi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#variable name not accurate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[1;31m# append results of current epoch to df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_nmi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Projects\\MV1_proxies\\proxy-nca-master\\proxy-nca-master\\utils.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, dataloader, with_nmi)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# get predictions by assigning nearest 8 neighbors with euclidian\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_by_euclidian_at_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Projects\\MV1_proxies\\proxy-nca-master\\proxy-nca-master\\evaluation\\recall.py\u001b[0m in \u001b[0;36massign_by_euclidian_at_k\u001b[1;34m(X, T, k)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# get nearest points\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistances\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlargest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mii\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lists of each parameter to search\n",
    "seeds = [0]\n",
    "lrs = [1]\n",
    "scaling_xs = [1.0]\n",
    "scaling_ps = [8.0]\n",
    "sz_embs = [64]\n",
    "sz_batches = [32,128]\n",
    "eds = [1,2]\n",
    "\n",
    "results = {}\n",
    "if os.path.exists(args.results_filename):\n",
    "    results_df = pd.read_csv(args.results_filename)\n",
    "    index = 0\n",
    "else:\n",
    "    results_df = pd.DataFrame(columns = ['index','epoch', 'r@1', 'r@2', 'r@4', 'r@8', 'NMI',\n",
    "                                         'lr','scl_x','scl_p','sz_emb','seed', 'edition',\n",
    "                                         'batch', 'torch version'])\n",
    "    index = 0\n",
    "for lr in lrs:\n",
    "    args.lr_proxynca = lr\n",
    "    for scl_x in scaling_xs:\n",
    "        args.scaling_x = scl_x\n",
    "        for scl_p in scaling_ps:\n",
    "            args.scaling_p = scl_p\n",
    "            for sz_emb in sz_embs:\n",
    "                args.sz_embeddings = sz_emb\n",
    "                for seed in seeds:\n",
    "                    args.seed = seed\n",
    "                    for ed in eds:\n",
    "                        args.edition = ed\n",
    "                        for sz_batch in sz_batches:\n",
    "                            args.sz_batch = sz_batch\n",
    "                            if (results_df[(results_df.lr == lr) & (results_df.scl_x == scl_x)\n",
    "                                               & (results_df.scl_p == scl_p)\n",
    "                                               & (results_df.sz_emb == sz_emb)\n",
    "                                               & (results_df.seed == seed)\n",
    "                                               & (results_df.edition == ed)\n",
    "                                               & (results_df['torch version'] == args.torch_version)\n",
    "                                               & (results_df['batch'] == sz_batch)]\n",
    "                                .shape[0] == 0):\n",
    "                                if results_df['index'].shape[0] > 0:\n",
    "                                    index = results_df['index'].max() + 1\n",
    "                                print(index)\n",
    "                                res_df = train_and_test()\n",
    "                                res_df['lr'] = lr\n",
    "                                res_df['scl_x'] = scl_x\n",
    "                                res_df['scl_p'] = scl_p\n",
    "                                res_df['index'] = index\n",
    "                                res_df['sz_emb'] = sz_emb\n",
    "                                res_df['seed'] = seed\n",
    "                                res_df['edition'] = ed\n",
    "                                res_df['batch'] = sz_batch\n",
    "                                res_df['torch version'] = args.torch_version\n",
    "                                results_df = pd.concat([results_df, res_df])\n",
    "                                results_df.to_csv(args.results_filename, index = False)\n",
    "                                index+=1\n",
    "                            else:\n",
    "                                print(f'skipped version:{index}. It was already done.')\n",
    "                                index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'folder'\n",
    "folder = 'train'\n",
    "os.path.join(root, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = setup_config()['dataset']['food']['root']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(root,folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "i = torchvision.datasets.ImageFolder(path).imgs[0]\n",
    "fn = os.path.split(i[0])[1]\n",
    "print(fn)\n",
    "# print(fn[:2])\n",
    "i[1]\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "511.96px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
