{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import imp\n",
    "import dataset\n",
    "import utils\n",
    "import proxynca\n",
    "import net\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from utils import JSONEncoder, json_dumps\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Args Class\n",
    "\n",
    "Class containing the parameters for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args():\n",
    "    dataset = 'food_test'\n",
    "    config = 'config1.json'\n",
    "    sz_embedding = 64 #size of the embedding that is appendet to inceptionv2\n",
    "    sz_batch = 32 #number of samples per batch\n",
    "    nb_epochs = 20\n",
    "    gpu_id = 0\n",
    "    nb_workers = 4\n",
    "    with_nmi = True  #turn calculations for nmi on or off turn off for sop\n",
    "    scaling_x = 3.0 #scaling factor for the normalized embeddings\n",
    "    scaling_p = 3.0 #scaling factor for the normalized proxies\n",
    "    lr_proxynca = 1.0 #learning rate for proxynca\n",
    "    log_filename = (f'''{dataset}-{dt.now().strftime(\"%Y%m%d-%H%M%S\")}''')\n",
    "    results_filename = f'{dataset}-results.csv'\n",
    "    torch_version = str(torch.__version__)\n",
    "    edition = 0\n",
    "    seed = 0\n",
    "    train_classes = None\n",
    "    eval_classes = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(args = args):\n",
    "    seed = args.seed\n",
    "    if seed != -1:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    else:\n",
    "        print('not seeded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(args.gpu_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_config(args = args):\n",
    "    config = utils.load_config(args.config)\n",
    "    config['criterion']['args']['scaling_x'] = args.scaling_x\n",
    "    config['criterion']['args']['scaling_p'] = args.scaling_p\n",
    "    config['opt']['args']['proxynca']['lr'] = args.lr_proxynca\n",
    "    if args.train_classes:\n",
    "        config['dataset'][args.dataset]['classes']['train'] = args.train_classes\n",
    "    if args.eval_classes:\n",
    "        config['dataset'][args.dataset]['classes']['eval'] = args.eval_classes\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_tr(config = setup_config(), args = args):\n",
    "    dl_tr = torch.utils.data.DataLoader(\n",
    "        dataset.load(name = args.dataset,\n",
    "                     root = config['dataset'][args.dataset]['root'],\n",
    "                     classes = config['dataset'][args.dataset]['classes']['train'],\n",
    "                     transform = dataset.utils.make_transform(**config['transform_parameters'])\n",
    "                    ),\n",
    "        batch_size = args.sz_batch,\n",
    "        shuffle = True,\n",
    "        num_workers = args.nb_workers,\n",
    "        drop_last = True,\n",
    "        pin_memory = True\n",
    "    )\n",
    "    return dl_tr\n",
    "\n",
    "def load_ev(config = setup_config(), args = args):\n",
    "    dl_ev = torch.utils.data.DataLoader(\n",
    "        dataset.load(\n",
    "            name = args.dataset,\n",
    "            root = config['dataset'][args.dataset]['root'],\n",
    "            classes = config['dataset'][args.dataset]['classes']['eval'],\n",
    "            transform = dataset.utils.make_transform(\n",
    "                **config['transform_parameters'],\n",
    "                is_train = False)\n",
    "        ),\n",
    "        batch_size = args.sz_batch,\n",
    "        shuffle = False,\n",
    "        num_workers = args.nb_workers,\n",
    "        pin_memory = True\n",
    "    )\n",
    "    return dl_ev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(args = args):\n",
    "    model = net.bn_inception(pretrained = True)\n",
    "    net.embed(model, sz_embedding = args.sz_embedding)\n",
    "    model = model.cuda()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion\n",
    "\n",
    "This function initializes the training class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_criterion(config = setup_config(), args = args, dl_tr = load_tr()):\n",
    "    criterion = proxynca.ProxyNCA(\n",
    "        nb_classes = dl_tr.dataset.nb_classes(),\n",
    "        sz_embedding = args.sz_embedding,\n",
    "        **config['criterion']['args']).cuda()\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_opt(config = setup_config(), model = setup_model(), criterion = setup_criterion()):\n",
    "    opt = config['opt']['type'](\n",
    "        [\n",
    "            { # inception parameters, excluding embedding layer\n",
    "                **{'params': list(\n",
    "                    set(\n",
    "                        model.parameters()\n",
    "                    ).difference(\n",
    "                        set(model.embedding_layer.parameters())\n",
    "                    )\n",
    "                )},\n",
    "                **config['opt']['args']['backbone']\n",
    "            },\n",
    "            { # embedding parameters\n",
    "                **{'params': model.embedding_layer.parameters()},\n",
    "                **config['opt']['args']['embedding']\n",
    "            },\n",
    "            { # proxy nca parameters\n",
    "                **{'params': criterion.parameters()},\n",
    "                **config['opt']['args']['proxynca']\n",
    "            }\n",
    "        ],\n",
    "        **config['opt']['args']['base']\n",
    "    )\n",
    "    return opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_scheduler(config = setup_config(), opt = setup_opt()):\n",
    "    scheduler = config['lr_scheduler']['type'](\n",
    "        opt, **config['lr_scheduler']['args'])\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(args = args):\n",
    "    imp.reload(logging)\n",
    "    logging.basicConfig(\n",
    "        format = \"%(asctime)s %(message)s\",\n",
    "        level = logging.INFO,\n",
    "        handlers = [\n",
    "            logging.FileHandler(\"{0}/{1}.log\".format('log', args.log_filename)),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logging.info(\"Training parameters: {}\".format(vars(args)))\n",
    "    logging.info(\"Training for {} epochs\".format(args.nb_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(args = args):\n",
    "    #set up new parameters\n",
    "    seed_everything(args)\n",
    "    config = setup_config(args)\n",
    "    dl_tr = load_tr(config, args)\n",
    "    dl_ev = load_ev(config, args)\n",
    "    model = setup_model(args = args)\n",
    "    criterion = setup_criterion(config = config, args = args, dl_tr = load_tr())\n",
    "    opt = setup_opt(config = config, model = model, criterion = criterion)\n",
    "    scheduler = setup_scheduler(config = config, opt = opt)\n",
    "    setup_logging(args = args)\n",
    "    \n",
    "    if args.with_nmi == True:\n",
    "        df = pd.DataFrame(columns = ['epoch', 'r@1', 'r@2', 'r@4', 'r@8', 'NMI'])\n",
    "    else:\n",
    "        df = pd.DataFrame(columns = ['epoch', 'r@1', 'r@2', 'r@4','r@8'])\n",
    "    \n",
    "    losses = []\n",
    "    t1 = time.time()\n",
    "    logging.info(\"**Evaluating initial model.**\")\n",
    "    with torch.no_grad():\n",
    "        utils.evaluate(model, dl_ev, with_nmi = args.with_nmi)\n",
    "\n",
    "    for e in range(0, args.nb_epochs):\n",
    "        if e!=0:\n",
    "            scheduler.step()\n",
    "        time_per_epoch_1 = time.time()\n",
    "        losses_per_epoch = []\n",
    "        for x,y, _ in dl_tr:\n",
    "            opt.zero_grad()\n",
    "            m = model(x.cuda())\n",
    "            loss = criterion(m, y.cuda())\n",
    "            loss.backward()\n",
    "\n",
    "#             torch.nn.utils.clip_grad_value_(model.parameters(), 10)\n",
    "            \n",
    "            losses_per_epoch.append(loss.data.cpu().numpy())\n",
    "            opt.step()\n",
    "\n",
    "        time_per_epoch_2 = time.time()\n",
    "        losses.append(np.mean(losses_per_epoch[-20:]))\n",
    "        logging.info(\n",
    "            \"Epoch: {}, loss: {:.3f}, time (seconds): {:.2f}.\".format(\n",
    "                e,\n",
    "                losses[-1],\n",
    "                time_per_epoch_2 - time_per_epoch_1))\n",
    "        with torch.no_grad():\n",
    "            logging.info(\"**Evaluating.**\")\n",
    "            recall = utils.evaluate(model, dl_ev, with_nmi = args.with_nmi) #variable name not accurate\n",
    "            # append results of current epoch to df\n",
    "            if args.with_nmi == True:\n",
    "                lst = recall[0].copy()\n",
    "                lst.append(recall[1])\n",
    "                lst.insert(0,e)\n",
    "                df_epoch = pd.DataFrame([lst], columns = ['epoch', 'r@1', 'r@2', 'r@4','r@8', 'NMI'])\n",
    "            else:\n",
    "                lst = recall.copy()\n",
    "                lst.insert(0,e)\n",
    "                df_epoch = pd.DataFrame([lst], columns = ['epoch', 'r@1', 'r@2', 'r@4', 'r@8'])\n",
    "            df = pd.concat([df,df_epoch])\n",
    "            model.losses = losses\n",
    "            model.current_epoch = e\n",
    "\n",
    "    t2 = time.time()\n",
    "    logging.info(\"Total training time (minutes): {:.2f}.\".format((t2 - t1) / 60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped version:0. It was already done.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-27 22:38:10,454 Training parameters: {'__module__': '__main__', 'dataset': 'food_test', 'config': 'config1.json', 'sz_embedding': 64, 'sz_batch': 32, 'nb_epochs': 20, 'gpu_id': 0, 'nb_workers': 4, 'with_nmi': True, 'scaling_x': 1.0, 'scaling_p': 3.0, 'lr_proxynca': 1, 'log_filename': 'food_test-20200927-223804', 'results_filename': 'food_test-results.csv', 'torch_version': '1.1.0', 'edition': 0, 'seed': 0, 'train_classes': range(0, 50), 'eval_classes': range(50, 101), '__dict__': <attribute '__dict__' of 'args' objects>, '__weakref__': <attribute '__weakref__' of 'args' objects>, '__doc__': None}\n",
      "2020-09-27 22:38:10,455 Training for 20 epochs\n",
      "2020-09-27 22:38:10,457 **Evaluating initial model.**\n",
      "2020-09-27 22:39:14,430 NMI: 25.916\n",
      "2020-09-27 22:39:26,269 R@1 : 32.493\n",
      "2020-09-27 22:39:26,521 R@2 : 41.514\n",
      "2020-09-27 22:39:26,774 R@4 : 51.994\n",
      "2020-09-27 22:39:27,027 R@8 : 62.736\n",
      "2020-09-27 22:40:24,125 Epoch: 0, loss: 2.488, time (seconds): 57.10.\n",
      "2020-09-27 22:40:24,126 **Evaluating.**\n",
      "2020-09-27 22:41:02,203 NMI: 29.915\n",
      "2020-09-27 22:41:12,841 R@1 : 37.681\n",
      "2020-09-27 22:41:13,090 R@2 : 48.310\n",
      "2020-09-27 22:41:13,343 R@4 : 58.799\n",
      "2020-09-27 22:41:13,596 R@8 : 68.680\n",
      "2020-09-27 22:42:06,770 Epoch: 1, loss: 2.242, time (seconds): 53.17.\n",
      "2020-09-27 22:42:06,770 **Evaluating.**\n",
      "2020-09-27 22:42:45,741 NMI: 29.774\n",
      "2020-09-27 22:42:57,594 R@1 : 38.116\n",
      "2020-09-27 22:42:57,850 R@2 : 48.657\n",
      "2020-09-27 22:42:58,103 R@4 : 58.764\n",
      "2020-09-27 22:42:58,354 R@8 : 68.871\n",
      "2020-09-27 22:43:51,445 Epoch: 2, loss: 2.107, time (seconds): 53.09.\n",
      "2020-09-27 22:43:51,446 **Evaluating.**\n",
      "2020-09-27 22:44:29,774 NMI: 29.813\n",
      "2020-09-27 22:44:41,183 R@1 : 37.212\n",
      "2020-09-27 22:44:41,438 R@2 : 47.910\n",
      "2020-09-27 22:44:41,688 R@4 : 58.643\n",
      "2020-09-27 22:44:41,943 R@8 : 68.610\n",
      "2020-09-27 22:45:34,762 Epoch: 3, loss: 2.058, time (seconds): 52.82.\n",
      "2020-09-27 22:45:34,762 **Evaluating.**\n",
      "2020-09-27 22:46:12,923 NMI: 29.027\n",
      "2020-09-27 22:46:24,441 R@1 : 36.352\n",
      "2020-09-27 22:46:24,689 R@2 : 46.363\n",
      "2020-09-27 22:46:24,938 R@4 : 57.087\n",
      "2020-09-27 22:46:25,186 R@8 : 67.090\n",
      "2020-09-27 22:47:17,762 Epoch: 4, loss: 1.885, time (seconds): 52.57.\n",
      "2020-09-27 22:47:17,763 **Evaluating.**\n",
      "2020-09-27 22:47:56,187 NMI: 27.519\n",
      "2020-09-27 22:48:07,794 R@1 : 35.848\n",
      "2020-09-27 22:48:08,058 R@2 : 45.338\n",
      "2020-09-27 22:48:08,309 R@4 : 55.653\n",
      "2020-09-27 22:48:08,562 R@8 : 65.725\n",
      "2020-09-27 22:49:02,287 Epoch: 5, loss: 1.921, time (seconds): 53.72.\n",
      "2020-09-27 22:49:02,288 **Evaluating.**\n",
      "2020-09-27 22:49:40,176 NMI: 26.883\n",
      "2020-09-27 22:49:51,676 R@1 : 35.135\n",
      "2020-09-27 22:49:51,927 R@2 : 44.842\n",
      "2020-09-27 22:49:52,185 R@4 : 54.888\n",
      "2020-09-27 22:49:52,436 R@8 : 65.430\n",
      "2020-09-27 22:50:45,635 Epoch: 6, loss: 1.722, time (seconds): 53.20.\n",
      "2020-09-27 22:50:45,636 **Evaluating.**\n",
      "2020-09-27 22:51:23,681 NMI: 26.598\n",
      "2020-09-27 22:51:35,411 R@1 : 35.040\n",
      "2020-09-27 22:51:35,669 R@2 : 45.129\n",
      "2020-09-27 22:51:35,922 R@4 : 55.836\n",
      "2020-09-27 22:51:36,175 R@8 : 65.586\n",
      "2020-09-27 22:52:29,699 Epoch: 7, loss: 1.686, time (seconds): 53.52.\n",
      "2020-09-27 22:52:29,700 **Evaluating.**\n",
      "2020-09-27 22:53:08,253 NMI: 27.170\n",
      "2020-09-27 22:53:19,876 R@1 : 35.013\n",
      "2020-09-27 22:53:20,130 R@2 : 44.947\n",
      "2020-09-27 22:53:20,387 R@4 : 55.227\n",
      "2020-09-27 22:53:20,642 R@8 : 64.934\n",
      "2020-09-27 22:54:13,502 Epoch: 8, loss: 1.615, time (seconds): 52.86.\n",
      "2020-09-27 22:54:13,503 **Evaluating.**\n",
      "2020-09-27 22:54:52,282 NMI: 25.733\n",
      "2020-09-27 22:55:03,952 R@1 : 33.545\n",
      "2020-09-27 22:55:04,223 R@2 : 44.104\n",
      "2020-09-27 22:55:04,499 R@4 : 53.958\n",
      "2020-09-27 22:55:04,750 R@8 : 64.170\n",
      "2020-09-27 22:56:01,772 Epoch: 9, loss: 1.662, time (seconds): 57.02.\n",
      "2020-09-27 22:56:01,773 **Evaluating.**\n",
      "2020-09-27 22:56:40,457 NMI: 26.272\n",
      "2020-09-27 22:56:52,098 R@1 : 33.945\n",
      "2020-09-27 22:56:52,346 R@2 : 44.086\n",
      "2020-09-27 22:56:52,597 R@4 : 54.489\n",
      "2020-09-27 22:56:52,846 R@8 : 65.013\n",
      "2020-09-27 22:57:45,404 Epoch: 10, loss: 1.543, time (seconds): 52.56.\n",
      "2020-09-27 22:57:45,405 **Evaluating.**\n",
      "2020-09-27 22:58:24,779 NMI: 26.243\n",
      "2020-09-27 22:58:36,740 R@1 : 33.432\n",
      "2020-09-27 22:58:37,002 R@2 : 43.669\n",
      "2020-09-27 22:58:37,254 R@4 : 53.985\n",
      "2020-09-27 22:58:37,506 R@8 : 64.370\n",
      "2020-09-27 22:59:31,357 Epoch: 11, loss: 1.479, time (seconds): 53.85.\n",
      "2020-09-27 22:59:31,357 **Evaluating.**\n",
      "2020-09-27 23:00:11,416 NMI: 25.184\n",
      "2020-09-27 23:00:23,250 R@1 : 32.763\n",
      "2020-09-27 23:00:23,511 R@2 : 42.930\n",
      "2020-09-27 23:00:23,767 R@4 : 53.089\n",
      "2020-09-27 23:00:24,015 R@8 : 62.814\n",
      "2020-09-27 23:01:18,830 Epoch: 12, loss: 1.441, time (seconds): 54.81.\n",
      "2020-09-27 23:01:18,831 **Evaluating.**\n",
      "2020-09-27 23:01:59,978 NMI: 24.554\n",
      "2020-09-27 23:02:12,050 R@1 : 31.998\n",
      "2020-09-27 23:02:12,300 R@2 : 41.722\n",
      "2020-09-27 23:02:12,553 R@4 : 52.629\n",
      "2020-09-27 23:02:12,804 R@8 : 63.005\n"
     ]
    }
   ],
   "source": [
    "# lists of each parameter to search\n",
    "seeds = [0]\n",
    "lrs = [1]\n",
    "scaling_xs = [1.0,3.0,8.0]\n",
    "scaling_ps = [1.0,3.0,8.0]\n",
    "sz_embs = [64]\n",
    "sz_batches = [32]\n",
    "eds = [0]\n",
    "train_rnges = [range(0,50)]\n",
    "eval_rnges = [range(50,101)]\n",
    "\n",
    "results = {}\n",
    "if os.path.exists(args.results_filename):\n",
    "    results_df = pd.read_csv(args.results_filename)\n",
    "    index = 0\n",
    "else:\n",
    "    results_df = pd.DataFrame(columns = ['index','epoch', 'r@1', 'r@2', 'r@4', 'r@8', 'NMI',\n",
    "                                         'lr','scl_x','scl_p','sz_emb','seed', 'edition',\n",
    "                                         'batch', 'torch version', 'train_classes', 'eval_classes'])\n",
    "    index = 0\n",
    "for lr in lrs:\n",
    "    args.lr_proxynca = lr\n",
    "    for scl_x in scaling_xs:\n",
    "        args.scaling_x = scl_x\n",
    "        for scl_p in scaling_ps:\n",
    "            args.scaling_p = scl_p\n",
    "            for sz_emb in sz_embs:\n",
    "                args.sz_embedding = sz_emb\n",
    "                for seed in seeds:\n",
    "                    args.seed = seed\n",
    "                    for ed in eds:\n",
    "                        args.edition = ed\n",
    "                        for sz_batch in sz_batches:\n",
    "                            args.sz_batch = sz_batch\n",
    "                            for train_rng in train_rnges:\n",
    "                                args.train_classes = train_rng\n",
    "                                tr_rng_str = str([train_rng[0], train_rng[-1]+1])\n",
    "                                for eval_rng in eval_rnges:\n",
    "                                    args.eval_classes = eval_rng\n",
    "                                    ev_rng_str = str([eval_rng[0], eval_rng[-1]+1])\n",
    "                                    if (results_df[(results_df.lr == lr) & (results_df.scl_x == scl_x)\n",
    "                                                       & (results_df.scl_p == scl_p)\n",
    "                                                       & (results_df.sz_emb == sz_emb)\n",
    "                                                       & (results_df.seed == seed)\n",
    "                                                       & (results_df.edition == ed)\n",
    "                                                       & (results_df['torch version'] == args.torch_version)\n",
    "                                                       & (results_df['batch'] == sz_batch)\n",
    "                                                       & (results_df['train_classes'] == tr_rng_str)\n",
    "                                                       & (results_df['eval_classes'] == ev_rng_str)]\n",
    "                                        .shape[0] == 0):\n",
    "                                        if results_df['index'].shape[0] > 0:\n",
    "                                            index = results_df['index'].max() + 1\n",
    "                                        print(index)\n",
    "                                        res_df = train_and_test()\n",
    "                                        res_df['lr'] = lr\n",
    "                                        res_df['scl_x'] = scl_x\n",
    "                                        res_df['scl_p'] = scl_p\n",
    "                                        res_df['index'] = index\n",
    "                                        res_df['sz_emb'] = sz_emb\n",
    "                                        res_df['seed'] = seed\n",
    "                                        res_df['edition'] = ed\n",
    "                                        res_df['batch'] = sz_batch\n",
    "                                        res_df['torch version'] = args.torch_version\n",
    "                                        res_df.loc[:,'train_classes'] = tr_rng_str\n",
    "                                        res_df.loc[:, 'eval_classes'] = ev_rng_str\n",
    "                                        results_df = pd.concat([results_df, res_df])\n",
    "                                        results_df.to_csv(args.results_filename, index = False)\n",
    "                                        index+=1\n",
    "                                    else:\n",
    "                                        print(f'skipped version:{index}. It was already done.')\n",
    "                                        index+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "511.946px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
